{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import list \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree \n",
    "from sklearn.linear_model import LogisticRegression #Import LogisticRegression \n",
    "from sklearn import svm #Import SVM \n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import KFold \n",
    " \n",
    "# Load dataset \n",
    "Data = pd.read_csv(\"breast-cancer-wisconsin.data\",names=['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']) \n",
    " \n",
    "Data = Data.apply(pd.to_numeric,errors='coerce').fillna(0) \n",
    " \n",
    "Data[\"Class\"] = Data.Class.map({2:0,4:1}) \n",
    " \n",
    "print(Data) \n",
    " \n",
    " \n",
    "def classification_Compare(data, model, K): \n",
    "    feature = data[['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape', 'Marginal Adhesion', \n",
    "                    'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses']] \n",
    "    feature = feature.to_numpy().astype(np.uint32) \n",
    "    label = data.Class \n",
    "    label = label.to_numpy().astype(np.uint32) \n",
    " \n",
    "    scaler = MinMaxScaler() \n",
    " \n",
    "    scaler.fit(feature) \n",
    " \n",
    "    kfold = KFold(n_splits=K) \n",
    " \n",
    "    cv_accuracy = [] \n",
    "    n_iter = 0 \n",
    " \n",
    "    for train_index, test_index in kfold.split(feature):  # Split the features data by the kfold number specified above \n",
    " \n",
    "        x_train, x_test = feature[train_index], feature[test_index] \n",
    "        y_train, y_test = label[train_index], label[test_index] \n",
    " \n",
    "        model.fit(x_train, y_train) \n",
    "        pred = model.predict(x_test) \n",
    "        n_iter += 1 \n",
    " \n",
    "        accuracy = np.round(accuracy_score(y_test, pred), 4)  # round to 4 decimal places \n",
    "        train_size = x_train.shape[0] \n",
    "        test_size = x_test.shape[0] \n",
    " \n",
    "        print('\\n#{0} Cross-validation accuracy : {1},  Training data size : {2},  Validation data size : {3}' \n",
    "              .format(n_iter, accuracy, train_size, test_size)) \n",
    "        cv_accuracy.append(accuracy) \n",
    " \n",
    "    for train_index, test_index in kfold.split(feature):  # Split Min_Max scaled featres data by kfold number specified above \n",
    " \n",
    "        x_train, x_test = feature[train_index], feature[test_index] \n",
    "        y_train, y_test = label[train_index], label[test_index] \n",
    " \n",
    "        x_train = scaler.transform(x_train) \n",
    "        x_test = scaler.transform(x_test) \n",
    " \n",
    "        model.fit(x_train, y_train) \n",
    "        pred = model.predict(x_test) \n",
    "        n_iter += 1 \n",
    " \n",
    "        accuracy = np.round(accuracy_score(y_test, pred), 4)  # round to 4 decimal places \n",
    "        train_size = x_train.shape[0] \n",
    "        test_size = x_test.shape[0] \n",
    " \n",
    "        print('\\n#{0} Min_Max_Scaler Cross-validation accuracy : {1},  Training data size : {2},  Validation data size : {3}' \n",
    "              .format(n_iter, accuracy, train_size, test_size)) \n",
    "        cv_accuracy.append(accuracy) \n",
    " \n",
    "    return cv_accuracy \n",
    " \n",
    " \n",
    "def make_model(K, data, case, **kwargs): \n",
    "    if (case == 1): \n",
    "        model = DecisionTreeClassifier(criterion=\"entropy\", splitter=kwargs.get('splitter', \"best\") \n",
    "                                       , max_depth=kwargs.get('max_depth', None) \n",
    "                                       , min_samples_split=kwargs.get('min_samples_split', 2) \n",
    "                                       , min_samples_leaf=kwargs.get('min_samples_leaf', 1) \n",
    "                                       , min_weight_fraction_leaf=kwargs.get('min_weight_fraction_leaf', 0.0) \n",
    "                                       , max_features=kwargs.get('max_features', None) \n",
    "                                       , random_state=kwargs.get('random_state', None) \n",
    "                                       , max_leaf_nodes=kwargs.get('max_leaf_nodes', None) \n",
    "                                       , min_impurity_decrease=kwargs.get('min_impurity_decrease', 0.0) \n",
    "                                       , class_weight=kwargs.get('class_weight', None) \n",
    "                                       , ccp_alpha=kwargs.get('ccp_alpha', 0.0)) \n",
    "        return classification_Compare(data, model, K) \n",
    "    elif (case == 2): \n",
    "        model = DecisionTreeClassifier(criterion=\"entropy\", splitter=kwargs.get('splitter', \"best\") \n",
    "                                       , max_depth=kwargs.get('max_depth', None) \n",
    "                                       , min_samples_split=kwargs.get('min_samples_split', 2) \n",
    "                                       , min_samples_leaf=kwargs.get('min_samples_leaf', 1) \n",
    "                                       , min_weight_fraction_leaf=kwargs.get('min_weight_fraction_leaf', 0.0) \n",
    "                                       , max_features=kwargs.get('max_features', None) \n",
    "                                       , random_state=kwargs.get('random_state', None) \n",
    "                                       , max_leaf_nodes=kwargs.get('max_leaf_nodes', None) \n",
    "                                       , min_impurity_decrease=kwargs.get('min_impurity_decrease', 0.0) \n",
    "                                       , class_weight=kwargs.get('class_weight', None) \n",
    "                                       , ccp_alpha=kwargs.get('ccp_alpha', 0.0)) \n",
    "        return classification_Compare(data, model, K) \n",
    "    elif (case == 3): \n",
    "        model = LogisticRegression(penalty=kwargs.get('penalty', 'l2') \n",
    "                                   , dual=kwargs.get('dual', False) \n",
    "                                   , tol=kwargs.get('tol', 1e-4) \n",
    "                                   , C=kwargs.get('C', 1.0) \n",
    "                                   , fit_intercept=kwargs.get('fit_intercept', True) \n",
    "                                   , intercept_scaling=kwargs.get('intercept_scaling', 1) \n",
    "                                   , class_weight=kwargs.get('class_weight', None) \n",
    "                                   , random_state=kwargs.get('random_state', None) \n",
    "                                   , solver=kwargs.get('solver', 'lbfgs') \n",
    "                                   , max_iter=kwargs.get('max_iter', 100) \n",
    "                                   , multi_class=kwargs.get('multi_class', 'auto') \n",
    "                                   , verbose=kwargs.get('verbose', 0) \n",
    "                                   , warm_start=kwargs.get('warm_start', False) \n",
    "                                   , n_jobs=kwargs.get('n_jobs', None) \n",
    "                                   , l1_ratio=kwargs.get('l1_ratio', None)) \n",
    "        return classification_Compare(data, model, K) \n",
    "    elif (case == 4): \n",
    "        model = svm.SVC(C=kwargs.get('C', 1.0), \n",
    "                        degree=kwargs.get('degree', 3), \n",
    "                        gamma=kwargs.get('gamma', 'scale'), \n",
    "                        coef0=kwargs.get('coef0', 0.0), \n",
    "                        shrinking=kwargs.get('shrinking', True), \n",
    "                        probability=kwargs.get('probability', False), \n",
    "                        tol=kwargs.get('tol', 1e-3), \n",
    "                        cache_size=kwargs.get('cache_size', 200), \n",
    "                        class_weight=kwargs.get('class_weight', None), \n",
    "                        verbose=kwargs.get('verbose', False), \n",
    "                        max_iter=kwargs.get('max_iter', -1), \n",
    "                        decision_function_shape=kwargs.get('decision_function_shape', 'ovr'), \n",
    "                        break_ties=kwargs.get('break_ties', False), \n",
    "                        random_state=kwargs.get('random_state', None)) \n",
    "        return classification_Compare(data, model, K) \n",
    "    else: \n",
    "        print('case error! (1~4)') \n",
    " \n",
    " \n",
    "acc_list = [] \n",
    " \n",
    "def acc_test(acc_list): \n",
    "    print('\\n<DecisionTreeClassifier_gini>') \n",
    "    tmp = make_model(3,Data,1,max_depth=10,min_samples_split=3,max_features='sqrt') \n",
    "    acc_list.append(tmp) \n",
    " \n",
    "    print('\\n\\n<DecisionTreeClassifier_entropy>') \n",
    "    tmp = make_model(3,Data,2,max_depth=7,min_samples_split=2,max_features='sqrt') \n",
    "    acc_list.append(tmp) \n",
    " \n",
    "    print('\\n\\n<LogisticRegression>') \n",
    "    tmp = make_model(3,Data,3,random_state=30,max_iter=100) \n",
    "    acc_list.append(tmp) \n",
    " \n",
    "    print('\\n\\n<SVM>') \n",
    "    tmp = make_model(3,Data,4,random_state=20,max_iter=10) \n",
    "    acc_list.append(tmp) \n",
    " \n",
    "acc_test(acc_list) \n",
    " \n",
    "# Print all Accuracy \n",
    "print('\\n<Accuracy output for all models> \\n\\n[DecisionTreeGini] [DecisionTreeEntropy] [LogisticRegression] [SVM]\\n') \n",
    "print(acc_list) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
